{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Win Probability Prediction with PCA and Role/Champion Analysis\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Data exploration and role/champion-specific performance analysis\n",
    "2. PCA feature reduction with consideration for role/champion context\n",
    "3. Training and comparison of multiple classification models\n",
    "4. Identifying outlier matches where predictions diverge from reality\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "**Challenge:** Performance metrics that indicate a \"good\" performance vary significantly by role and champion:\n",
    "- Support players have different typical stats than carries\n",
    "- Tank champions have different damage/vision patterns than assassins\n",
    "- Raw stats don't account for these contextual differences\n",
    "\n",
    "**Approach:** We'll explore multiple strategies to handle this:\n",
    "1. **Role-specific normalization**: Normalize features within each role\n",
    "2. **Role/champion as features**: Include role/champion as categorical features\n",
    "3. **Separate models**: Train separate models for different roles/champions\n",
    "4. **Combined approach**: Use PCA on normalized features + role context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import asyncio\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print('Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup database connection\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from lol_data_center.database.engine import get_async_session\n",
    "from lol_data_center.ml.data_extraction import MatchDataExtractor\n",
    "\n",
    "print('Database modules loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Extraction and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract match data\n",
    "async def load_data():\n",
    "    async with get_async_session() as session:\n",
    "        extractor = MatchDataExtractor(session)\n",
    "        df = await extractor.extract_match_features()\n",
    "        champion_stats = await extractor.get_champion_stats()\n",
    "        role_stats = await extractor.get_role_stats()\n",
    "    return df, champion_stats, role_stats\n",
    "\n",
    "df, champion_stats, role_stats = await load_data()\n",
    "\n",
    "print(f'Loaded {len(df)} match records')\n",
    "print(f'\\nDataset shape: {df.shape}')\n",
    "print(f'\\nColumns: {list(df.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data exploration\n",
    "print('Dataset Info:')\n",
    "print(df.info())\n",
    "print('\\n' + '='*80 + '\\n')\n",
    "print('First few rows:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Win rate distribution\n",
    "win_rate = df['win'].mean()\n",
    "print(f'Overall win rate: {win_rate:.2%}')\n",
    "\n",
    "# Visualize win distribution\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Win rate by role\n",
    "if 'team_position' in df.columns:\n",
    "    role_wins = df.groupby('team_position')['win'].agg(['mean', 'count'])\n",
    "    role_wins.columns = ['win_rate', 'count']\n",
    "    role_wins = role_wins[role_wins['count'] >= 10]  # Filter roles with few games\n",
    "    \n",
    "    ax[0].bar(role_wins.index, role_wins['win_rate'])\n",
    "    ax[0].set_title('Win Rate by Role')\n",
    "    ax[0].set_xlabel('Role')\n",
    "    ax[0].set_ylabel('Win Rate')\n",
    "    ax[0].axhline(y=0.5, color='r', linestyle='--', label='50%')\n",
    "    ax[0].legend()\n",
    "    ax[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Top 15 champions by games played\n",
    "if 'champion_name' in df.columns:\n",
    "    champ_wins = df.groupby('champion_name')['win'].agg(['mean', 'count'])\n",
    "    champ_wins.columns = ['win_rate', 'count']\n",
    "    top_champs = champ_wins.nlargest(15, 'count')\n",
    "    \n",
    "    ax[1].barh(range(len(top_champs)), top_champs['win_rate'])\n",
    "    ax[1].set_yticks(range(len(top_champs)))\n",
    "    ax[1].set_yticklabels(top_champs.index)\n",
    "    ax[1].set_title('Win Rate - Top 15 Champions by Games')\n",
    "    ax[1].set_xlabel('Win Rate')\n",
    "    ax[1].axvline(x=0.5, color='r', linestyle='--', label='50%')\n",
    "    ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Role and Champion Context Analysis\n",
    "\n",
    "Here we analyze how performance metrics vary by role and champion to understand the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze key metrics by role\n",
    "key_metrics = ['damage_per_min', 'gold_per_min', 'cs_per_min', 'vision_score_per_min', 'kda']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(key_metrics):\n",
    "    if metric in df.columns and 'team_position' in df.columns:\n",
    "        df.boxplot(column=metric, by='team_position', ax=axes[idx])\n",
    "        axes[idx].set_title(f'{metric} by Role')\n",
    "        axes[idx].set_xlabel('Role')\n",
    "        axes[idx].set_ylabel(metric)\n",
    "        plt.sca(axes[idx])\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "# Remove the last subplot if we have 6 subplots but only 5 metrics\n",
    "fig.delaxes(axes[-1])\n",
    "\n",
    "plt.suptitle('Performance Metrics Vary Significantly by Role', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nKey Observation: Different roles have very different stat distributions!')\n",
    "print('This confirms the need for role-aware feature engineering.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison of roles\n",
    "if 'team_position' in df.columns:\n",
    "    role_stats_detailed = df.groupby('team_position')[key_metrics].agg(['mean', 'std']).round(2)\n",
    "    print('Detailed Role Statistics:')\n",
    "    print(role_stats_detailed)\n",
    "    print('\\nCoefficient of Variation (CV) by metric across roles:')\n",
    "    for metric in key_metrics:\n",
    "        if metric in df.columns:\n",
    "            role_means = df.groupby('team_position')[metric].mean()\n",
    "            cv = role_means.std() / role_means.mean()\n",
    "            print(f'{metric}: {cv:.2%} variation across roles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering with Role Context\n",
    "\n",
    "We'll create multiple feature sets to compare approaches:\n",
    "1. **Raw features**: No normalization\n",
    "2. **Role-normalized features**: Z-score normalization within each role\n",
    "3. **Features + role encoding**: Include role as a categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling (exclude identifiers and target)\n",
    "exclude_cols = ['match_id', 'puuid', 'champion_id', 'champion_name', \n",
    "                'team_position', 'individual_position', 'win']\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "print(f'Selected {len(feature_cols)} features for modeling:')\n",
    "print(feature_cols)\n",
    "\n",
    "# Prepare feature sets\n",
    "X_raw = df[feature_cols].copy()\n",
    "y = df['win'].astype(int)\n",
    "\n",
    "print(f'\\nFeature matrix shape: {X_raw.shape}')\n",
    "print(f'Target distribution: {y.value_counts().to_dict()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create role-normalized features\n",
    "X_role_normalized = X_raw.copy().astype(float)  # Ensure we have float type for normalization\n",
    "\n",
    "if 'team_position' in df.columns:\n",
    "    for role in df['team_position'].unique():\n",
    "        role_mask = df['team_position'] == role\n",
    "        \n",
    "        # Normalize each feature within this role\n",
    "        for col in feature_cols:\n",
    "            role_data = X_role_normalized.loc[role_mask, col]\n",
    "            mean = role_data.mean()\n",
    "            std = role_data.std()\n",
    "            \n",
    "            if std > 0:  # Avoid division by zero\n",
    "                X_role_normalized.loc[role_mask, col] = (role_data - mean) / std\n",
    "    \n",
    "    print('Created role-normalized features')\n",
    "else:\n",
    "    print('No role information available - using standard normalization')\n",
    "    scaler = StandardScaler()\n",
    "    X_role_normalized = pd.DataFrame(\n",
    "        scaler.fit_transform(X_raw),\n",
    "        columns=X_raw.columns,\n",
    "        index=X_raw.index\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features with role encoding\n",
    "X_with_role = X_raw.copy()\n",
    "\n",
    "if 'team_position' in df.columns and 'champion_name' in df.columns:\n",
    "    # One-hot encode role\n",
    "    role_dummies = pd.get_dummies(df['team_position'], prefix='role')\n",
    "    X_with_role = pd.concat([X_with_role, role_dummies], axis=1)\n",
    "    \n",
    "    # Encode champion (use label encoding to avoid too many features)\n",
    "    le_champion = LabelEncoder()\n",
    "    X_with_role['champion_encoded'] = le_champion.fit_transform(df['champion_name'])\n",
    "    \n",
    "    print(f'Features with role encoding shape: {X_with_role.shape}')\n",
    "    print(f'Added {len(role_dummies.columns)} role features and 1 champion encoding')\n",
    "else:\n",
    "    print('Role/champion information not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PCA Feature Reduction\n",
    "\n",
    "We'll apply PCA to reduce dimensionality while preserving variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features before PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_raw)\n",
    "X_role_norm_scaled = scaler.fit_transform(X_role_normalized)\n",
    "\n",
    "# Fit PCA to determine optimal number of components\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "# Plot explained variance\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scree plot\n",
    "axes[0].plot(range(1, len(pca_full.explained_variance_ratio_) + 1),\n",
    "             pca_full.explained_variance_ratio_, 'bo-')\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0].set_title('Scree Plot')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Cumulative variance\n",
    "axes[1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'ro-')\n",
    "axes[1].axhline(y=0.95, color='g', linestyle='--', label='95% variance')\n",
    "axes[1].axhline(y=0.90, color='b', linestyle='--', label='90% variance')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1].set_title('Cumulative Explained Variance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find number of components for 90% and 95% variance\n",
    "n_components_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(f'Components needed for 90% variance: {n_components_90}')\n",
    "print(f'Components needed for 95% variance: {n_components_95}')\n",
    "print(f'Original features: {X_raw.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA with optimal number of components (using 95% variance)\n",
    "n_components = min(n_components_95, 15)  # Cap at 15 for interpretability\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "pca_role = PCA(n_components=n_components)\n",
    "X_pca_role_norm = pca_role.fit_transform(X_role_norm_scaled)\n",
    "\n",
    "print(f'Reduced from {X_raw.shape[1]} to {n_components} features')\n",
    "print(f'Explained variance: {pca.explained_variance_ratio_.sum():.2%}')\n",
    "print(f'\\nPCA-transformed data shape: {X_pca.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first two principal components colored by win\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Raw features PCA\n",
    "scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='RdYlGn', alpha=0.5, s=10)\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} var)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} var)')\n",
    "axes[0].set_title('PCA - Raw Features')\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Win')\n",
    "\n",
    "# Role-normalized PCA\n",
    "scatter2 = axes[1].scatter(X_pca_role_norm[:, 0], X_pca_role_norm[:, 1], \n",
    "                          c=y, cmap='RdYlGn', alpha=0.5, s=10)\n",
    "axes[1].set_xlabel(f'PC1 ({pca_role.explained_variance_ratio_[0]:.1%} var)')\n",
    "axes[1].set_ylabel(f'PC2 ({pca_role.explained_variance_ratio_[1]:.1%} var)')\n",
    "axes[1].set_title('PCA - Role-Normalized Features')\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Win')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Comparison\n",
    "\n",
    "We'll compare multiple models:\n",
    "1. Logistic Regression (baseline)\n",
    "2. Random Forest\n",
    "3. SVM (Support Vector Machine)\n",
    "\n",
    "For each model, we'll test:\n",
    "- Raw features\n",
    "- PCA-reduced features\n",
    "- Role-normalized + PCA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "\n",
    "# Create multiple train/test splits for different feature sets\n",
    "datasets = {\n",
    "    'Raw Features': (X_scaled, y),\n",
    "    'PCA (Raw)': (X_pca, y),\n",
    "    'PCA (Role-Normalized)': (X_pca_role_norm, y),\n",
    "}\n",
    "\n",
    "splits = {}\n",
    "for name, (X, y_data) in datasets.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_data, test_size=test_size, random_state=random_state, stratify=y_data\n",
    "    )\n",
    "    splits[name] = (X_train, X_test, y_train, y_test)\n",
    "    print(f'{name}: Train={X_train.shape}, Test={X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=random_state),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=random_state, n_jobs=-1),\n",
    "    'SVM': SVC(probability=True, random_state=random_state),\n",
    "}\n",
    "\n",
    "# Train and evaluate all combinations\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f'\\nTraining {model_name}...')\n",
    "    \n",
    "    for dataset_name, (X_train, X_test, y_train, y_test) in splits.items():\n",
    "        print(f'  - {dataset_name}...', end=' ')\n",
    "        \n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Evaluate\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        # Cross-validation score\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "        cv_mean = cv_scores.mean()\n",
    "        \n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Dataset': dataset_name,\n",
    "            'Accuracy': accuracy,\n",
    "            'ROC-AUC': roc_auc,\n",
    "            'CV Accuracy': cv_mean,\n",
    "        })\n",
    "        \n",
    "        print(f'Acc={accuracy:.3f}, AUC={roc_auc:.3f}, CV={cv_mean:.3f}')\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print('\\n' + '='*80)\n",
    "print('RESULTS SUMMARY')\n",
    "print('='*80)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "pivot_acc = results_df.pivot(index='Model', columns='Dataset', values='Accuracy')\n",
    "pivot_acc.plot(kind='bar', ax=axes[0], rot=0)\n",
    "axes[0].set_title('Model Accuracy Comparison')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend(title='Dataset', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[0].set_ylim([0.5, 1.0])\n",
    "axes[0].axhline(y=0.5, color='r', linestyle='--', alpha=0.3, label='Baseline (50%)')\n",
    "\n",
    "# ROC-AUC comparison\n",
    "pivot_auc = results_df.pivot(index='Model', columns='Dataset', values='ROC-AUC')\n",
    "pivot_auc.plot(kind='bar', ax=axes[1], rot=0)\n",
    "axes[1].set_title('Model ROC-AUC Comparison')\n",
    "axes[1].set_ylabel('ROC-AUC')\n",
    "axes[1].legend(title='Dataset', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[1].set_ylim([0.5, 1.0])\n",
    "axes[1].axhline(y=0.5, color='r', linestyle='--', alpha=0.3, label='Baseline (50%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Model Analysis\n",
    "\n",
    "Let's analyze the best performing model in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model\n",
    "best_idx = results_df['ROC-AUC'].idxmax()\n",
    "best_model_name = results_df.loc[best_idx, 'Model']\n",
    "best_dataset_name = results_df.loc[best_idx, 'Dataset']\n",
    "best_roc_auc = results_df.loc[best_idx, 'ROC-AUC']\n",
    "\n",
    "print(f'Best Model: {best_model_name}')\n",
    "print(f'Best Dataset: {best_dataset_name}')\n",
    "print(f'ROC-AUC: {best_roc_auc:.3f}')\n",
    "\n",
    "# Retrain best model\n",
    "best_model = models[best_model_name]\n",
    "X_train, X_test, y_train, y_test = splits[best_dataset_name]\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=['Loss', 'Win']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and ROC Curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Loss', 'Win'], yticklabels=['Loss', 'Win'])\n",
    "axes[0].set_title('Confusion Matrix')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "axes[1].plot(fpr, tpr, label=f'ROC curve (AUC = {best_roc_auc:.3f})', linewidth=2)\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='Random (AUC = 0.5)')\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curve')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (if Random Forest)\n",
    "if best_model_name == 'Random Forest' and best_dataset_name == 'Raw Features':\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False).head(15)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(feature_importance)), feature_importance['importance'])\n",
    "    plt.yticks(range(len(feature_importance)), feature_importance['feature'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top 15 Feature Importances (Random Forest)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\nTop 10 Most Important Features:')\n",
    "    print(feature_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Outlier Detection\n",
    "\n",
    "Identify matches where the prediction diverges significantly from reality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions for full dataset using best model\n",
    "# Use the test set for outlier analysis\n",
    "predictions_df = pd.DataFrame({\n",
    "    'win': y_test.values,\n",
    "    'win_probability': y_pred_proba,\n",
    "    'predicted_win': y_pred,\n",
    "})\n",
    "\n",
    "# Identify outliers\n",
    "threshold = 0.7  # High confidence threshold\n",
    "\n",
    "# Unexpected wins: won but had low win probability\n",
    "unexpected_wins = (predictions_df['win'] == 1) & (predictions_df['win_probability'] < (1 - threshold))\n",
    "\n",
    "# Unexpected losses: lost but had high win probability\n",
    "unexpected_losses = (predictions_df['win'] == 0) & (predictions_df['win_probability'] > threshold)\n",
    "\n",
    "outliers = predictions_df[unexpected_wins | unexpected_losses].copy()\n",
    "outliers['outlier_type'] = np.where(outliers['win'], 'Unexpected Win', 'Unexpected Loss')\n",
    "outliers['surprise_score'] = np.where(\n",
    "    outliers['win'],\n",
    "    1 - outliers['win_probability'],\n",
    "    outliers['win_probability']\n",
    ")\n",
    "\n",
    "outliers = outliers.sort_values('surprise_score', ascending=False)\n",
    "\n",
    "print(f'Total outliers: {len(outliers)} ({len(outliers)/len(predictions_df)*100:.1f}%)')\n",
    "print(f'Unexpected wins: {unexpected_wins.sum()}')\n",
    "print(f'Unexpected losses: {unexpected_losses.sum()}')\n",
    "print('\\nTop 10 Most Surprising Matches:')\n",
    "print(outliers.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Probability distribution for wins and losses\n",
    "wins_proba = predictions_df[predictions_df['win'] == 1]['win_probability']\n",
    "losses_proba = predictions_df[predictions_df['win'] == 0]['win_probability']\n",
    "\n",
    "axes[0].hist(wins_proba, bins=30, alpha=0.6, label='Actual Wins', color='green', edgecolor='black')\n",
    "axes[0].hist(losses_proba, bins=30, alpha=0.6, label='Actual Losses', color='red', edgecolor='black')\n",
    "axes[0].axvline(x=threshold, color='blue', linestyle='--', label=f'Threshold ({threshold})')\n",
    "axes[0].axvline(x=1-threshold, color='blue', linestyle='--')\n",
    "axes[0].set_xlabel('Predicted Win Probability')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of Win Probabilities')\n",
    "axes[0].legend()\n",
    "\n",
    "# Outlier types\n",
    "outlier_counts = outliers['outlier_type'].value_counts()\n",
    "axes[1].bar(outlier_counts.index, outlier_counts.values, color=['green', 'red'])\n",
    "axes[1].set_title('Outlier Types')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xlabel('Outlier Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Findings and Recommendations\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Role Context Matters**: Performance metrics vary significantly across roles (e.g., supports have lower damage, higher vision scores)\n",
    "\n",
    "2. **PCA Effectiveness**: PCA successfully reduces dimensionality while retaining most variance. Role-normalized features may provide different perspectives.\n",
    "\n",
    "3. **Model Performance**: [Results will show which model performs best]\n",
    "\n",
    "4. **Outlier Detection**: The model can identify surprising match outcomes where performance metrics didn't align with the result\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. **Feature Engineering Strategy**:\n",
    "   - Use role-normalized features when role context is important\n",
    "   - Consider training separate models per role for maximum accuracy\n",
    "   - Alternatively, include role as a categorical feature in a unified model\n",
    "\n",
    "2. **Model Selection**:\n",
    "   - Random Forest tends to perform well with mixed feature types\n",
    "   - Logistic Regression provides interpretability\n",
    "   - Choose based on accuracy vs. interpretability trade-off\n",
    "\n",
    "3. **PCA Usage**:\n",
    "   - Beneficial for visualization and reducing overfitting\n",
    "   - May sacrifice some interpretability\n",
    "   - Consider using 90-95% variance threshold\n",
    "\n",
    "4. **Implementation Path**:\n",
    "   - Save the best model, scaler, and PCA transformer\n",
    "   - Create API for real-time win probability prediction\n",
    "   - Implement role-specific prediction pipelines if needed\n",
    "   - Use outlier detection to highlight unusual match outcomes\n",
    "\n",
    "5. **Future Improvements**:\n",
    "   - Collect more data for better generalization\n",
    "   - Experiment with deep learning models\n",
    "   - Add temporal features (player improvement over time)\n",
    "   - Include team composition features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and associated artifacts\n",
    "import pickle\n",
    "\n",
    "model_dir = Path('../models')\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Determine which scaler and PCA to save based on best dataset\n",
    "if best_dataset_name == 'PCA (Role-Normalized)':\n",
    "    scaler_to_save = StandardScaler()\n",
    "    scaler_to_save.fit(X_role_normalized)\n",
    "    pca_to_save = pca_role\n",
    "elif best_dataset_name == 'PCA (Raw)':\n",
    "    scaler_to_save = StandardScaler()\n",
    "    scaler_to_save.fit(X_raw)\n",
    "    pca_to_save = pca\n",
    "else:  # Raw Features\n",
    "    scaler_to_save = StandardScaler()\n",
    "    scaler_to_save.fit(X_raw)\n",
    "    pca_to_save = None\n",
    "\n",
    "# Save model\n",
    "model_path = model_dir / 'win_probability_model.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'model': best_model,\n",
    "        'feature_names': feature_cols,\n",
    "        'model_name': best_model_name,\n",
    "        'dataset_name': best_dataset_name,\n",
    "    }, f)\n",
    "print(f'Saved model to {model_path}')\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = model_dir / 'scaler.pkl'\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler_to_save, f)\n",
    "print(f'Saved scaler to {scaler_path}')\n",
    "\n",
    "# Save PCA if used\n",
    "if pca_to_save is not None:\n",
    "    pca_path = model_dir / 'pca.pkl'\n",
    "    with open(pca_path, 'wb') as f:\n",
    "        pickle.dump(pca_to_save, f)\n",
    "    print(f'Saved PCA to {pca_path}')\n",
    "\n",
    "print('\\nModel artifacts saved successfully!')\n",
    "print(f'Best configuration: {best_model_name} with {best_dataset_name}')\n",
    "print(f'ROC-AUC: {best_roc_auc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated:\n",
    "1. ✅ Data extraction and role/champion-specific analysis\n",
    "2. ✅ PCA feature reduction with role context considerations\n",
    "3. ✅ Comparison of multiple classification models (Logistic Regression, Random Forest, SVM)\n",
    "4. ✅ Outlier detection for identifying surprising match outcomes\n",
    "5. ✅ Model persistence for deployment\n",
    "\n",
    "The trained model can now be used to predict win probabilities for new matches, accounting for the role and champion context that makes \"good stats\" different across different positions and characters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
